{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product of two numbers with tensorflow. For example let's multiply 5 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder('int64', name=\"input_a\")\n",
    "b = tf.placeholder('int64', name=\"input_b\")\n",
    "result = a * b\n",
    "\n",
    "print(sess.run(result, {a: 7, b: 12}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/2a/ca1d592d30856cf0d72fa3fb581f4c7fef81511411283ca412e9c12af769/scikit_learn-0.20.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (8.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.2MB 146kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/site-packages (from scikit-learn->sklearn)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/kirill/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.0 sklearn-0.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 3.6592\n",
      "loss at iter 1: 3.4575\n",
      "loss at iter 2: 3.2671\n",
      "loss at iter 3: 3.0872\n",
      "loss at iter 4: 2.9174\n",
      "loss at iter 5: 2.7570\n",
      "loss at iter 6: 2.6056\n",
      "loss at iter 7: 2.4626\n",
      "loss at iter 8: 2.3276\n",
      "loss at iter 9: 2.2001\n",
      "loss at iter 10: 2.0797\n",
      "loss at iter 11: 1.9660\n",
      "loss at iter 12: 1.8586\n",
      "loss at iter 13: 1.7573\n",
      "loss at iter 14: 1.6615\n",
      "loss at iter 15: 1.5711\n",
      "loss at iter 16: 1.4857\n",
      "loss at iter 17: 1.4051\n",
      "loss at iter 18: 1.3290\n",
      "loss at iter 19: 1.2571\n",
      "loss at iter 20: 1.1892\n",
      "loss at iter 21: 1.1251\n",
      "loss at iter 22: 1.0645\n",
      "loss at iter 23: 1.0073\n",
      "loss at iter 24: 0.9533\n",
      "loss at iter 25: 0.9023\n",
      "loss at iter 26: 0.8541\n",
      "loss at iter 27: 0.8086\n",
      "loss at iter 28: 0.7657\n",
      "loss at iter 29: 0.7251\n",
      "loss at iter 30: 0.6868\n",
      "loss at iter 31: 0.6506\n",
      "loss at iter 32: 0.6164\n",
      "loss at iter 33: 0.5841\n",
      "loss at iter 34: 0.5536\n",
      "loss at iter 35: 0.5248\n",
      "loss at iter 36: 0.4976\n",
      "loss at iter 37: 0.4718\n",
      "loss at iter 38: 0.4476\n",
      "loss at iter 39: 0.4246\n",
      "loss at iter 40: 0.4030\n",
      "loss at iter 41: 0.3825\n",
      "loss at iter 42: 0.3632\n",
      "loss at iter 43: 0.3449\n",
      "loss at iter 44: 0.3276\n",
      "loss at iter 45: 0.3113\n",
      "loss at iter 46: 0.2959\n",
      "loss at iter 47: 0.2814\n",
      "loss at iter 48: 0.2676\n",
      "loss at iter 49: 0.2546\n",
      "loss at iter 50: 0.2424\n",
      "loss at iter 51: 0.2307\n",
      "loss at iter 52: 0.2198\n",
      "loss at iter 53: 0.2094\n",
      "loss at iter 54: 0.1996\n",
      "loss at iter 55: 0.1904\n",
      "loss at iter 56: 0.1816\n",
      "loss at iter 57: 0.1733\n",
      "loss at iter 58: 0.1655\n",
      "loss at iter 59: 0.1581\n",
      "loss at iter 60: 0.1511\n",
      "loss at iter 61: 0.1445\n",
      "loss at iter 62: 0.1383\n",
      "loss at iter 63: 0.1324\n",
      "loss at iter 64: 0.1268\n",
      "loss at iter 65: 0.1215\n",
      "loss at iter 66: 0.1165\n",
      "loss at iter 67: 0.1118\n",
      "loss at iter 68: 0.1073\n",
      "loss at iter 69: 0.1031\n",
      "loss at iter 70: 0.0991\n",
      "loss at iter 71: 0.0953\n",
      "loss at iter 72: 0.0917\n",
      "loss at iter 73: 0.0883\n",
      "loss at iter 74: 0.0851\n",
      "loss at iter 75: 0.0820\n",
      "loss at iter 76: 0.0792\n",
      "loss at iter 77: 0.0764\n",
      "loss at iter 78: 0.0739\n",
      "loss at iter 79: 0.0714\n",
      "loss at iter 80: 0.0691\n",
      "loss at iter 81: 0.0669\n",
      "loss at iter 82: 0.0648\n",
      "loss at iter 83: 0.0628\n",
      "loss at iter 84: 0.0609\n",
      "loss at iter 85: 0.0592\n",
      "loss at iter 86: 0.0575\n",
      "loss at iter 87: 0.0559\n",
      "loss at iter 88: 0.0544\n",
      "loss at iter 89: 0.0529\n",
      "loss at iter 90: 0.0515\n",
      "loss at iter 91: 0.0502\n",
      "loss at iter 92: 0.0490\n",
      "loss at iter 93: 0.0478\n",
      "loss at iter 94: 0.0467\n",
      "loss at iter 95: 0.0457\n",
      "loss at iter 96: 0.0446\n",
      "loss at iter 97: 0.0437\n",
      "loss at iter 98: 0.0428\n",
      "loss at iter 99: 0.0419\n",
      "loss at iter 100: 0.0411\n",
      "loss at iter 101: 0.0403\n",
      "loss at iter 102: 0.0395\n",
      "loss at iter 103: 0.0388\n",
      "loss at iter 104: 0.0381\n",
      "loss at iter 105: 0.0375\n",
      "loss at iter 106: 0.0368\n",
      "loss at iter 107: 0.0362\n",
      "loss at iter 108: 0.0357\n",
      "loss at iter 109: 0.0351\n",
      "loss at iter 110: 0.0346\n",
      "loss at iter 111: 0.0341\n",
      "loss at iter 112: 0.0336\n",
      "loss at iter 113: 0.0331\n",
      "loss at iter 114: 0.0327\n",
      "loss at iter 115: 0.0323\n",
      "loss at iter 116: 0.0319\n",
      "loss at iter 117: 0.0315\n",
      "loss at iter 118: 0.0311\n",
      "loss at iter 119: 0.0307\n",
      "loss at iter 120: 0.0304\n",
      "loss at iter 121: 0.0301\n",
      "loss at iter 122: 0.0297\n",
      "loss at iter 123: 0.0294\n",
      "loss at iter 124: 0.0291\n",
      "loss at iter 125: 0.0288\n",
      "loss at iter 126: 0.0286\n",
      "loss at iter 127: 0.0283\n",
      "loss at iter 128: 0.0280\n",
      "loss at iter 129: 0.0278\n",
      "loss at iter 130: 0.0275\n",
      "loss at iter 131: 0.0273\n",
      "loss at iter 132: 0.0270\n",
      "loss at iter 133: 0.0268\n",
      "loss at iter 134: 0.0266\n",
      "loss at iter 135: 0.0264\n",
      "loss at iter 136: 0.0262\n",
      "loss at iter 137: 0.0260\n",
      "loss at iter 138: 0.0258\n",
      "loss at iter 139: 0.0256\n",
      "loss at iter 140: 0.0254\n",
      "loss at iter 141: 0.0252\n",
      "loss at iter 142: 0.0250\n",
      "loss at iter 143: 0.0249\n",
      "loss at iter 144: 0.0247\n",
      "loss at iter 145: 0.0245\n",
      "loss at iter 146: 0.0244\n",
      "loss at iter 147: 0.0242\n",
      "loss at iter 148: 0.0240\n",
      "loss at iter 149: 0.0239\n",
      "loss at iter 150: 0.0237\n",
      "loss at iter 151: 0.0236\n",
      "loss at iter 152: 0.0234\n",
      "loss at iter 153: 0.0233\n",
      "loss at iter 154: 0.0231\n",
      "loss at iter 155: 0.0230\n",
      "loss at iter 156: 0.0229\n",
      "loss at iter 157: 0.0227\n",
      "loss at iter 158: 0.0226\n",
      "loss at iter 159: 0.0225\n",
      "loss at iter 160: 0.0223\n",
      "loss at iter 161: 0.0222\n",
      "loss at iter 162: 0.0221\n",
      "loss at iter 163: 0.0220\n",
      "loss at iter 164: 0.0218\n",
      "loss at iter 165: 0.0217\n",
      "loss at iter 166: 0.0216\n",
      "loss at iter 167: 0.0215\n",
      "loss at iter 168: 0.0214\n",
      "loss at iter 169: 0.0212\n",
      "loss at iter 170: 0.0211\n",
      "loss at iter 171: 0.0210\n",
      "loss at iter 172: 0.0209\n",
      "loss at iter 173: 0.0208\n",
      "loss at iter 174: 0.0207\n",
      "loss at iter 175: 0.0206\n",
      "loss at iter 176: 0.0205\n",
      "loss at iter 177: 0.0204\n",
      "loss at iter 178: 0.0203\n",
      "loss at iter 179: 0.0202\n",
      "loss at iter 180: 0.0200\n",
      "loss at iter 181: 0.0199\n",
      "loss at iter 182: 0.0198\n",
      "loss at iter 183: 0.0197\n",
      "loss at iter 184: 0.0196\n",
      "loss at iter 185: 0.0195\n",
      "loss at iter 186: 0.0194\n",
      "loss at iter 187: 0.0193\n",
      "loss at iter 188: 0.0192\n",
      "loss at iter 189: 0.0191\n",
      "loss at iter 190: 0.0191\n",
      "loss at iter 191: 0.0190\n",
      "loss at iter 192: 0.0189\n",
      "loss at iter 193: 0.0188\n",
      "loss at iter 194: 0.0187\n",
      "loss at iter 195: 0.0186\n",
      "loss at iter 196: 0.0185\n",
      "loss at iter 197: 0.0184\n",
      "loss at iter 198: 0.0183\n",
      "loss at iter 199: 0.0182\n",
      "loss at iter 200: 0.0181\n",
      "loss at iter 201: 0.0180\n",
      "loss at iter 202: 0.0179\n",
      "loss at iter 203: 0.0179\n",
      "loss at iter 204: 0.0178\n",
      "loss at iter 205: 0.0177\n",
      "loss at iter 206: 0.0176\n",
      "loss at iter 207: 0.0175\n",
      "loss at iter 208: 0.0174\n",
      "loss at iter 209: 0.0173\n",
      "loss at iter 210: 0.0173\n",
      "loss at iter 211: 0.0172\n",
      "loss at iter 212: 0.0171\n",
      "loss at iter 213: 0.0170\n",
      "loss at iter 214: 0.0169\n",
      "loss at iter 215: 0.0168\n",
      "loss at iter 216: 0.0168\n",
      "loss at iter 217: 0.0167\n",
      "loss at iter 218: 0.0166\n",
      "loss at iter 219: 0.0165\n",
      "loss at iter 220: 0.0164\n",
      "loss at iter 221: 0.0163\n",
      "loss at iter 222: 0.0163\n",
      "loss at iter 223: 0.0162\n",
      "loss at iter 224: 0.0161\n",
      "loss at iter 225: 0.0160\n",
      "loss at iter 226: 0.0160\n",
      "loss at iter 227: 0.0159\n",
      "loss at iter 228: 0.0158\n",
      "loss at iter 229: 0.0157\n",
      "loss at iter 230: 0.0157\n",
      "loss at iter 231: 0.0156\n",
      "loss at iter 232: 0.0155\n",
      "loss at iter 233: 0.0154\n",
      "loss at iter 234: 0.0154\n",
      "loss at iter 235: 0.0153\n",
      "loss at iter 236: 0.0152\n",
      "loss at iter 237: 0.0151\n",
      "loss at iter 238: 0.0151\n",
      "loss at iter 239: 0.0150\n",
      "loss at iter 240: 0.0149\n",
      "loss at iter 241: 0.0148\n",
      "loss at iter 242: 0.0148\n",
      "loss at iter 243: 0.0147\n",
      "loss at iter 244: 0.0146\n",
      "loss at iter 245: 0.0146\n",
      "loss at iter 246: 0.0145\n",
      "loss at iter 247: 0.0144\n",
      "loss at iter 248: 0.0143\n",
      "loss at iter 249: 0.0143\n",
      "loss at iter 250: 0.0142\n",
      "loss at iter 251: 0.0141\n",
      "loss at iter 252: 0.0141\n",
      "loss at iter 253: 0.0140\n",
      "loss at iter 254: 0.0139\n",
      "loss at iter 255: 0.0139\n",
      "loss at iter 256: 0.0138\n",
      "loss at iter 257: 0.0137\n",
      "loss at iter 258: 0.0137\n",
      "loss at iter 259: 0.0136\n",
      "loss at iter 260: 0.0135\n",
      "loss at iter 261: 0.0135\n",
      "loss at iter 262: 0.0134\n",
      "loss at iter 263: 0.0133\n",
      "loss at iter 264: 0.0133\n",
      "loss at iter 265: 0.0132\n",
      "loss at iter 266: 0.0132\n",
      "loss at iter 267: 0.0131\n",
      "loss at iter 268: 0.0130\n",
      "loss at iter 269: 0.0130\n",
      "loss at iter 270: 0.0129\n",
      "loss at iter 271: 0.0128\n",
      "loss at iter 272: 0.0128\n",
      "loss at iter 273: 0.0127\n",
      "loss at iter 274: 0.0127\n",
      "loss at iter 275: 0.0126\n",
      "loss at iter 276: 0.0125\n",
      "loss at iter 277: 0.0125\n",
      "loss at iter 278: 0.0124\n",
      "loss at iter 279: 0.0124\n",
      "loss at iter 280: 0.0123\n",
      "loss at iter 281: 0.0122\n",
      "loss at iter 282: 0.0122\n",
      "loss at iter 283: 0.0121\n",
      "loss at iter 284: 0.0121\n",
      "loss at iter 285: 0.0120\n",
      "loss at iter 286: 0.0120\n",
      "loss at iter 287: 0.0119\n",
      "loss at iter 288: 0.0118\n",
      "loss at iter 289: 0.0118\n",
      "loss at iter 290: 0.0117\n",
      "loss at iter 291: 0.0117\n",
      "loss at iter 292: 0.0116\n",
      "loss at iter 293: 0.0116\n",
      "loss at iter 294: 0.0115\n",
      "loss at iter 295: 0.0115\n",
      "loss at iter 296: 0.0114\n",
      "loss at iter 297: 0.0113\n",
      "loss at iter 298: 0.0113\n",
      "loss at iter 299: 0.0112\n",
      "loss at iter 300: 0.0112\n",
      "loss at iter 301: 0.0111\n",
      "loss at iter 302: 0.0111\n",
      "loss at iter 303: 0.0110\n",
      "loss at iter 304: 0.0110\n",
      "loss at iter 305: 0.0109\n",
      "loss at iter 306: 0.0109\n",
      "loss at iter 307: 0.0108\n",
      "loss at iter 308: 0.0108\n",
      "loss at iter 309: 0.0107\n",
      "loss at iter 310: 0.0107\n",
      "loss at iter 311: 0.0106\n",
      "loss at iter 312: 0.0106\n",
      "loss at iter 313: 0.0105\n",
      "loss at iter 314: 0.0105\n",
      "loss at iter 315: 0.0104\n",
      "loss at iter 316: 0.0104\n",
      "loss at iter 317: 0.0103\n",
      "loss at iter 318: 0.0103\n",
      "loss at iter 319: 0.0102\n",
      "loss at iter 320: 0.0102\n",
      "loss at iter 321: 0.0101\n",
      "loss at iter 322: 0.0101\n",
      "loss at iter 323: 0.0100\n",
      "loss at iter 324: 0.0100\n",
      "loss at iter 325: 0.0099\n",
      "loss at iter 326: 0.0099\n",
      "loss at iter 327: 0.0098\n",
      "loss at iter 328: 0.0098\n",
      "loss at iter 329: 0.0098\n",
      "loss at iter 330: 0.0097\n",
      "loss at iter 331: 0.0097\n",
      "loss at iter 332: 0.0096\n",
      "loss at iter 333: 0.0096\n",
      "loss at iter 334: 0.0095\n",
      "loss at iter 335: 0.0095\n",
      "loss at iter 336: 0.0094\n",
      "loss at iter 337: 0.0094\n",
      "loss at iter 338: 0.0094\n",
      "loss at iter 339: 0.0093\n",
      "loss at iter 340: 0.0093\n",
      "loss at iter 341: 0.0092\n",
      "loss at iter 342: 0.0092\n",
      "loss at iter 343: 0.0091\n",
      "loss at iter 344: 0.0091\n",
      "loss at iter 345: 0.0090\n",
      "loss at iter 346: 0.0090\n",
      "loss at iter 347: 0.0090\n",
      "loss at iter 348: 0.0089\n",
      "loss at iter 349: 0.0089\n",
      "loss at iter 350: 0.0088\n",
      "loss at iter 351: 0.0088\n",
      "loss at iter 352: 0.0088\n",
      "loss at iter 353: 0.0087\n",
      "loss at iter 354: 0.0087\n",
      "loss at iter 355: 0.0086\n",
      "loss at iter 356: 0.0086\n",
      "loss at iter 357: 0.0086\n",
      "loss at iter 358: 0.0085\n",
      "loss at iter 359: 0.0085\n",
      "loss at iter 360: 0.0084\n",
      "loss at iter 361: 0.0084\n",
      "loss at iter 362: 0.0084\n",
      "loss at iter 363: 0.0083\n",
      "loss at iter 364: 0.0083\n",
      "loss at iter 365: 0.0082\n",
      "loss at iter 366: 0.0082\n",
      "loss at iter 367: 0.0082\n",
      "loss at iter 368: 0.0081\n",
      "loss at iter 369: 0.0081\n",
      "loss at iter 370: 0.0081\n",
      "loss at iter 371: 0.0080\n",
      "loss at iter 372: 0.0080\n",
      "loss at iter 373: 0.0079\n",
      "loss at iter 374: 0.0079\n",
      "loss at iter 375: 0.0079\n",
      "loss at iter 376: 0.0078\n",
      "loss at iter 377: 0.0078\n",
      "loss at iter 378: 0.0078\n",
      "loss at iter 379: 0.0077\n",
      "loss at iter 380: 0.0077\n",
      "loss at iter 381: 0.0077\n",
      "loss at iter 382: 0.0076\n",
      "loss at iter 383: 0.0076\n",
      "loss at iter 384: 0.0075\n",
      "loss at iter 385: 0.0075\n",
      "loss at iter 386: 0.0075\n",
      "loss at iter 387: 0.0074\n",
      "loss at iter 388: 0.0074\n",
      "loss at iter 389: 0.0074\n",
      "loss at iter 390: 0.0073\n",
      "loss at iter 391: 0.0073\n",
      "loss at iter 392: 0.0073\n",
      "loss at iter 393: 0.0072\n",
      "loss at iter 394: 0.0072\n",
      "loss at iter 395: 0.0072\n",
      "loss at iter 396: 0.0071\n",
      "loss at iter 397: 0.0071\n",
      "loss at iter 398: 0.0071\n",
      "loss at iter 399: 0.0070\n",
      "loss at iter 400: 0.0070\n",
      "loss at iter 401: 0.0070\n",
      "loss at iter 402: 0.0069\n",
      "loss at iter 403: 0.0069\n",
      "loss at iter 404: 0.0069\n",
      "loss at iter 405: 0.0069\n",
      "loss at iter 406: 0.0068\n",
      "loss at iter 407: 0.0068\n",
      "loss at iter 408: 0.0068\n",
      "loss at iter 409: 0.0067\n",
      "loss at iter 410: 0.0067\n",
      "loss at iter 411: 0.0067\n",
      "loss at iter 412: 0.0066\n",
      "loss at iter 413: 0.0066\n",
      "loss at iter 414: 0.0066\n",
      "loss at iter 415: 0.0065\n",
      "loss at iter 416: 0.0065\n",
      "loss at iter 417: 0.0065\n",
      "loss at iter 418: 0.0065\n",
      "loss at iter 419: 0.0064\n",
      "loss at iter 420: 0.0064\n",
      "loss at iter 421: 0.0064\n",
      "loss at iter 422: 0.0063\n",
      "loss at iter 423: 0.0063\n",
      "loss at iter 424: 0.0063\n",
      "loss at iter 425: 0.0063\n",
      "loss at iter 426: 0.0062\n",
      "loss at iter 427: 0.0062\n",
      "loss at iter 428: 0.0062\n",
      "loss at iter 429: 0.0061\n",
      "loss at iter 430: 0.0061\n",
      "loss at iter 431: 0.0061\n",
      "loss at iter 432: 0.0061\n",
      "loss at iter 433: 0.0060\n",
      "loss at iter 434: 0.0060\n",
      "loss at iter 435: 0.0060\n",
      "loss at iter 436: 0.0059\n",
      "loss at iter 437: 0.0059\n",
      "loss at iter 438: 0.0059\n",
      "loss at iter 439: 0.0059\n",
      "loss at iter 440: 0.0058\n",
      "loss at iter 441: 0.0058\n",
      "loss at iter 442: 0.0058\n",
      "loss at iter 443: 0.0058\n",
      "loss at iter 444: 0.0057\n",
      "loss at iter 445: 0.0057\n",
      "loss at iter 446: 0.0057\n",
      "loss at iter 447: 0.0057\n",
      "loss at iter 448: 0.0056\n",
      "loss at iter 449: 0.0056\n",
      "loss at iter 450: 0.0056\n",
      "loss at iter 451: 0.0056\n",
      "loss at iter 452: 0.0055\n",
      "loss at iter 453: 0.0055\n",
      "loss at iter 454: 0.0055\n",
      "loss at iter 455: 0.0055\n",
      "loss at iter 456: 0.0054\n",
      "loss at iter 457: 0.0054\n",
      "loss at iter 458: 0.0054\n",
      "loss at iter 459: 0.0054\n",
      "loss at iter 460: 0.0053\n",
      "loss at iter 461: 0.0053\n",
      "loss at iter 462: 0.0053\n",
      "loss at iter 463: 0.0053\n",
      "loss at iter 464: 0.0052\n",
      "loss at iter 465: 0.0052\n",
      "loss at iter 466: 0.0052\n",
      "loss at iter 467: 0.0052\n",
      "loss at iter 468: 0.0052\n",
      "loss at iter 469: 0.0051\n",
      "loss at iter 470: 0.0051\n",
      "loss at iter 471: 0.0051\n",
      "loss at iter 472: 0.0051\n",
      "loss at iter 473: 0.0050\n",
      "loss at iter 474: 0.0050\n",
      "loss at iter 475: 0.0050\n",
      "loss at iter 476: 0.0050\n",
      "loss at iter 477: 0.0050\n",
      "loss at iter 478: 0.0049\n",
      "loss at iter 479: 0.0049\n",
      "loss at iter 480: 0.0049\n",
      "loss at iter 481: 0.0049\n",
      "loss at iter 482: 0.0048\n",
      "loss at iter 483: 0.0048\n",
      "loss at iter 484: 0.0048\n",
      "loss at iter 485: 0.0048\n",
      "loss at iter 486: 0.0048\n",
      "loss at iter 487: 0.0047\n",
      "loss at iter 488: 0.0047\n",
      "loss at iter 489: 0.0047\n",
      "loss at iter 490: 0.0047\n",
      "loss at iter 491: 0.0047\n",
      "loss at iter 492: 0.0046\n",
      "loss at iter 493: 0.0046\n",
      "loss at iter 494: 0.0046\n",
      "loss at iter 495: 0.0046\n",
      "loss at iter 496: 0.0046\n",
      "loss at iter 497: 0.0045\n",
      "loss at iter 498: 0.0045\n",
      "loss at iter 499: 0.0045\n",
      "loss at iter 500: 0.0045\n",
      "loss at iter 501: 0.0045\n",
      "loss at iter 502: 0.0044\n",
      "loss at iter 503: 0.0044\n",
      "loss at iter 504: 0.0044\n",
      "loss at iter 505: 0.0044\n",
      "loss at iter 506: 0.0044\n",
      "loss at iter 507: 0.0043\n",
      "loss at iter 508: 0.0043\n",
      "loss at iter 509: 0.0043\n",
      "loss at iter 510: 0.0043\n",
      "loss at iter 511: 0.0043\n",
      "loss at iter 512: 0.0042\n",
      "loss at iter 513: 0.0042\n",
      "loss at iter 514: 0.0042\n",
      "loss at iter 515: 0.0042\n",
      "loss at iter 516: 0.0042\n",
      "loss at iter 517: 0.0042\n",
      "loss at iter 518: 0.0041\n",
      "loss at iter 519: 0.0041\n",
      "loss at iter 520: 0.0041\n",
      "loss at iter 521: 0.0041\n",
      "loss at iter 522: 0.0041\n",
      "loss at iter 523: 0.0041\n",
      "loss at iter 524: 0.0040\n",
      "loss at iter 525: 0.0040\n",
      "loss at iter 526: 0.0040\n",
      "loss at iter 527: 0.0040\n",
      "loss at iter 528: 0.0040\n",
      "loss at iter 529: 0.0039\n",
      "loss at iter 530: 0.0039\n",
      "loss at iter 531: 0.0039\n",
      "loss at iter 532: 0.0039\n",
      "loss at iter 533: 0.0039\n",
      "loss at iter 534: 0.0039\n",
      "loss at iter 535: 0.0039\n",
      "loss at iter 536: 0.0038\n",
      "loss at iter 537: 0.0038\n",
      "loss at iter 538: 0.0038\n",
      "loss at iter 539: 0.0038\n",
      "loss at iter 540: 0.0038\n",
      "loss at iter 541: 0.0038\n",
      "loss at iter 542: 0.0037\n",
      "loss at iter 543: 0.0037\n",
      "loss at iter 544: 0.0037\n",
      "loss at iter 545: 0.0037\n",
      "loss at iter 546: 0.0037\n",
      "loss at iter 547: 0.0037\n",
      "loss at iter 548: 0.0036\n",
      "loss at iter 549: 0.0036\n",
      "loss at iter 550: 0.0036\n",
      "loss at iter 551: 0.0036\n",
      "loss at iter 552: 0.0036\n",
      "loss at iter 553: 0.0036\n",
      "loss at iter 554: 0.0036\n",
      "loss at iter 555: 0.0035\n",
      "loss at iter 556: 0.0035\n",
      "loss at iter 557: 0.0035\n",
      "loss at iter 558: 0.0035\n",
      "loss at iter 559: 0.0035\n",
      "loss at iter 560: 0.0035\n",
      "loss at iter 561: 0.0035\n",
      "loss at iter 562: 0.0034\n",
      "loss at iter 563: 0.0034\n",
      "loss at iter 564: 0.0034\n",
      "loss at iter 565: 0.0034\n",
      "loss at iter 566: 0.0034\n",
      "loss at iter 567: 0.0034\n",
      "loss at iter 568: 0.0034\n",
      "loss at iter 569: 0.0033\n",
      "loss at iter 570: 0.0033\n",
      "loss at iter 571: 0.0033\n",
      "loss at iter 572: 0.0033\n",
      "loss at iter 573: 0.0033\n",
      "loss at iter 574: 0.0033\n",
      "loss at iter 575: 0.0033\n",
      "loss at iter 576: 0.0032\n",
      "loss at iter 577: 0.0032\n",
      "loss at iter 578: 0.0032\n",
      "loss at iter 579: 0.0032\n",
      "loss at iter 580: 0.0032\n",
      "loss at iter 581: 0.0032\n",
      "loss at iter 582: 0.0032\n",
      "loss at iter 583: 0.0032\n",
      "loss at iter 584: 0.0031\n",
      "loss at iter 585: 0.0031\n",
      "loss at iter 586: 0.0031\n",
      "loss at iter 587: 0.0031\n",
      "loss at iter 588: 0.0031\n",
      "loss at iter 589: 0.0031\n",
      "loss at iter 590: 0.0031\n",
      "loss at iter 591: 0.0031\n",
      "loss at iter 592: 0.0030\n",
      "loss at iter 593: 0.0030\n",
      "loss at iter 594: 0.0030\n",
      "loss at iter 595: 0.0030\n",
      "loss at iter 596: 0.0030\n",
      "loss at iter 597: 0.0030\n",
      "loss at iter 598: 0.0030\n",
      "loss at iter 599: 0.0030\n",
      "loss at iter 600: 0.0029\n",
      "loss at iter 601: 0.0029\n",
      "loss at iter 602: 0.0029\n",
      "loss at iter 603: 0.0029\n",
      "loss at iter 604: 0.0029\n",
      "loss at iter 605: 0.0029\n",
      "loss at iter 606: 0.0029\n",
      "loss at iter 607: 0.0029\n",
      "loss at iter 608: 0.0028\n",
      "loss at iter 609: 0.0028\n",
      "loss at iter 610: 0.0028\n",
      "loss at iter 611: 0.0028\n",
      "loss at iter 612: 0.0028\n",
      "loss at iter 613: 0.0028\n",
      "loss at iter 614: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 615: 0.0028\n",
      "loss at iter 616: 0.0028\n",
      "loss at iter 617: 0.0027\n",
      "loss at iter 618: 0.0027\n",
      "loss at iter 619: 0.0027\n",
      "loss at iter 620: 0.0027\n",
      "loss at iter 621: 0.0027\n",
      "loss at iter 622: 0.0027\n",
      "loss at iter 623: 0.0027\n",
      "loss at iter 624: 0.0027\n",
      "loss at iter 625: 0.0027\n",
      "loss at iter 626: 0.0027\n",
      "loss at iter 627: 0.0026\n",
      "loss at iter 628: 0.0026\n",
      "loss at iter 629: 0.0026\n",
      "loss at iter 630: 0.0026\n",
      "loss at iter 631: 0.0026\n",
      "loss at iter 632: 0.0026\n",
      "loss at iter 633: 0.0026\n",
      "loss at iter 634: 0.0026\n",
      "loss at iter 635: 0.0026\n",
      "loss at iter 636: 0.0026\n",
      "loss at iter 637: 0.0025\n",
      "loss at iter 638: 0.0025\n",
      "loss at iter 639: 0.0025\n",
      "loss at iter 640: 0.0025\n",
      "loss at iter 641: 0.0025\n",
      "loss at iter 642: 0.0025\n",
      "loss at iter 643: 0.0025\n",
      "loss at iter 644: 0.0025\n",
      "loss at iter 645: 0.0025\n",
      "loss at iter 646: 0.0025\n",
      "loss at iter 647: 0.0024\n",
      "loss at iter 648: 0.0024\n",
      "loss at iter 649: 0.0024\n",
      "loss at iter 650: 0.0024\n",
      "loss at iter 651: 0.0024\n",
      "loss at iter 652: 0.0024\n",
      "loss at iter 653: 0.0024\n",
      "loss at iter 654: 0.0024\n",
      "loss at iter 655: 0.0024\n",
      "loss at iter 656: 0.0024\n",
      "loss at iter 657: 0.0024\n",
      "loss at iter 658: 0.0023\n",
      "loss at iter 659: 0.0023\n",
      "loss at iter 660: 0.0023\n",
      "loss at iter 661: 0.0023\n",
      "loss at iter 662: 0.0023\n",
      "loss at iter 663: 0.0023\n",
      "loss at iter 664: 0.0023\n",
      "loss at iter 665: 0.0023\n",
      "loss at iter 666: 0.0023\n",
      "loss at iter 667: 0.0023\n",
      "loss at iter 668: 0.0023\n",
      "loss at iter 669: 0.0023\n",
      "loss at iter 670: 0.0022\n",
      "loss at iter 671: 0.0022\n",
      "loss at iter 672: 0.0022\n",
      "loss at iter 673: 0.0022\n",
      "loss at iter 674: 0.0022\n",
      "loss at iter 675: 0.0022\n",
      "loss at iter 676: 0.0022\n",
      "loss at iter 677: 0.0022\n",
      "loss at iter 678: 0.0022\n",
      "loss at iter 679: 0.0022\n",
      "loss at iter 680: 0.0022\n",
      "loss at iter 681: 0.0022\n",
      "loss at iter 682: 0.0021\n",
      "loss at iter 683: 0.0021\n",
      "loss at iter 684: 0.0021\n",
      "loss at iter 685: 0.0021\n",
      "loss at iter 686: 0.0021\n",
      "loss at iter 687: 0.0021\n",
      "loss at iter 688: 0.0021\n",
      "loss at iter 689: 0.0021\n",
      "loss at iter 690: 0.0021\n",
      "loss at iter 691: 0.0021\n",
      "loss at iter 692: 0.0021\n",
      "loss at iter 693: 0.0021\n",
      "loss at iter 694: 0.0021\n",
      "loss at iter 695: 0.0020\n",
      "loss at iter 696: 0.0020\n",
      "loss at iter 697: 0.0020\n",
      "loss at iter 698: 0.0020\n",
      "loss at iter 699: 0.0020\n",
      "loss at iter 700: 0.0020\n",
      "loss at iter 701: 0.0020\n",
      "loss at iter 702: 0.0020\n",
      "loss at iter 703: 0.0020\n",
      "loss at iter 704: 0.0020\n",
      "loss at iter 705: 0.0020\n",
      "loss at iter 706: 0.0020\n",
      "loss at iter 707: 0.0020\n",
      "loss at iter 708: 0.0020\n",
      "loss at iter 709: 0.0019\n",
      "loss at iter 710: 0.0019\n",
      "loss at iter 711: 0.0019\n",
      "loss at iter 712: 0.0019\n",
      "loss at iter 713: 0.0019\n",
      "loss at iter 714: 0.0019\n",
      "loss at iter 715: 0.0019\n",
      "loss at iter 716: 0.0019\n",
      "loss at iter 717: 0.0019\n",
      "loss at iter 718: 0.0019\n",
      "loss at iter 719: 0.0019\n",
      "loss at iter 720: 0.0019\n",
      "loss at iter 721: 0.0019\n",
      "loss at iter 722: 0.0019\n",
      "loss at iter 723: 0.0019\n",
      "loss at iter 724: 0.0018\n",
      "loss at iter 725: 0.0018\n",
      "loss at iter 726: 0.0018\n",
      "loss at iter 727: 0.0018\n",
      "loss at iter 728: 0.0018\n",
      "loss at iter 729: 0.0018\n",
      "loss at iter 730: 0.0018\n",
      "loss at iter 731: 0.0018\n",
      "loss at iter 732: 0.0018\n",
      "loss at iter 733: 0.0018\n",
      "loss at iter 734: 0.0018\n",
      "loss at iter 735: 0.0018\n",
      "loss at iter 736: 0.0018\n",
      "loss at iter 737: 0.0018\n",
      "loss at iter 738: 0.0018\n",
      "loss at iter 739: 0.0018\n",
      "loss at iter 740: 0.0018\n",
      "loss at iter 741: 0.0017\n",
      "loss at iter 742: 0.0017\n",
      "loss at iter 743: 0.0017\n",
      "loss at iter 744: 0.0017\n",
      "loss at iter 745: 0.0017\n",
      "loss at iter 746: 0.0017\n",
      "loss at iter 747: 0.0017\n",
      "loss at iter 748: 0.0017\n",
      "loss at iter 749: 0.0017\n",
      "loss at iter 750: 0.0017\n",
      "loss at iter 751: 0.0017\n",
      "loss at iter 752: 0.0017\n",
      "loss at iter 753: 0.0017\n",
      "loss at iter 754: 0.0017\n",
      "loss at iter 755: 0.0017\n",
      "loss at iter 756: 0.0017\n",
      "loss at iter 757: 0.0017\n",
      "loss at iter 758: 0.0017\n",
      "loss at iter 759: 0.0016\n",
      "loss at iter 760: 0.0016\n",
      "loss at iter 761: 0.0016\n",
      "loss at iter 762: 0.0016\n",
      "loss at iter 763: 0.0016\n",
      "loss at iter 764: 0.0016\n",
      "loss at iter 765: 0.0016\n",
      "loss at iter 766: 0.0016\n",
      "loss at iter 767: 0.0016\n",
      "loss at iter 768: 0.0016\n",
      "loss at iter 769: 0.0016\n",
      "loss at iter 770: 0.0016\n",
      "loss at iter 771: 0.0016\n",
      "loss at iter 772: 0.0016\n",
      "loss at iter 773: 0.0016\n",
      "loss at iter 774: 0.0016\n",
      "loss at iter 775: 0.0016\n",
      "loss at iter 776: 0.0016\n",
      "loss at iter 777: 0.0016\n",
      "loss at iter 778: 0.0015\n",
      "loss at iter 779: 0.0015\n",
      "loss at iter 780: 0.0015\n",
      "loss at iter 781: 0.0015\n",
      "loss at iter 782: 0.0015\n",
      "loss at iter 783: 0.0015\n",
      "loss at iter 784: 0.0015\n",
      "loss at iter 785: 0.0015\n",
      "loss at iter 786: 0.0015\n",
      "loss at iter 787: 0.0015\n",
      "loss at iter 788: 0.0015\n",
      "loss at iter 789: 0.0015\n",
      "loss at iter 790: 0.0015\n",
      "loss at iter 791: 0.0015\n",
      "loss at iter 792: 0.0015\n",
      "loss at iter 793: 0.0015\n",
      "loss at iter 794: 0.0015\n",
      "loss at iter 795: 0.0015\n",
      "loss at iter 796: 0.0015\n",
      "loss at iter 797: 0.0015\n",
      "loss at iter 798: 0.0015\n",
      "loss at iter 799: 0.0015\n",
      "loss at iter 800: 0.0014\n",
      "loss at iter 801: 0.0014\n",
      "loss at iter 802: 0.0014\n",
      "loss at iter 803: 0.0014\n",
      "loss at iter 804: 0.0014\n",
      "loss at iter 805: 0.0014\n",
      "loss at iter 806: 0.0014\n",
      "loss at iter 807: 0.0014\n",
      "loss at iter 808: 0.0014\n",
      "loss at iter 809: 0.0014\n",
      "loss at iter 810: 0.0014\n",
      "loss at iter 811: 0.0014\n",
      "loss at iter 812: 0.0014\n",
      "loss at iter 813: 0.0014\n",
      "loss at iter 814: 0.0014\n",
      "loss at iter 815: 0.0014\n",
      "loss at iter 816: 0.0014\n",
      "loss at iter 817: 0.0014\n",
      "loss at iter 818: 0.0014\n",
      "loss at iter 819: 0.0014\n",
      "loss at iter 820: 0.0014\n",
      "loss at iter 821: 0.0014\n",
      "loss at iter 822: 0.0014\n",
      "loss at iter 823: 0.0014\n",
      "loss at iter 824: 0.0014\n",
      "loss at iter 825: 0.0013\n",
      "loss at iter 826: 0.0013\n",
      "loss at iter 827: 0.0013\n",
      "loss at iter 828: 0.0013\n",
      "loss at iter 829: 0.0013\n",
      "loss at iter 830: 0.0013\n",
      "loss at iter 831: 0.0013\n",
      "loss at iter 832: 0.0013\n",
      "loss at iter 833: 0.0013\n",
      "loss at iter 834: 0.0013\n",
      "loss at iter 835: 0.0013\n",
      "loss at iter 836: 0.0013\n",
      "loss at iter 837: 0.0013\n",
      "loss at iter 838: 0.0013\n",
      "loss at iter 839: 0.0013\n",
      "loss at iter 840: 0.0013\n",
      "loss at iter 841: 0.0013\n",
      "loss at iter 842: 0.0013\n",
      "loss at iter 843: 0.0013\n",
      "loss at iter 844: 0.0013\n",
      "loss at iter 845: 0.0013\n",
      "loss at iter 846: 0.0013\n",
      "loss at iter 847: 0.0013\n",
      "loss at iter 848: 0.0013\n",
      "loss at iter 849: 0.0013\n",
      "loss at iter 850: 0.0013\n",
      "loss at iter 851: 0.0013\n",
      "loss at iter 852: 0.0012\n",
      "loss at iter 853: 0.0012\n",
      "loss at iter 854: 0.0012\n",
      "loss at iter 855: 0.0012\n",
      "loss at iter 856: 0.0012\n",
      "loss at iter 857: 0.0012\n",
      "loss at iter 858: 0.0012\n",
      "loss at iter 859: 0.0012\n",
      "loss at iter 860: 0.0012\n",
      "loss at iter 861: 0.0012\n",
      "loss at iter 862: 0.0012\n",
      "loss at iter 863: 0.0012\n",
      "loss at iter 864: 0.0012\n",
      "loss at iter 865: 0.0012\n",
      "loss at iter 866: 0.0012\n",
      "loss at iter 867: 0.0012\n",
      "loss at iter 868: 0.0012\n",
      "loss at iter 869: 0.0012\n",
      "loss at iter 870: 0.0012\n",
      "loss at iter 871: 0.0012\n",
      "loss at iter 872: 0.0012\n",
      "loss at iter 873: 0.0012\n",
      "loss at iter 874: 0.0012\n",
      "loss at iter 875: 0.0012\n",
      "loss at iter 876: 0.0012\n",
      "loss at iter 877: 0.0012\n",
      "loss at iter 878: 0.0012\n",
      "loss at iter 879: 0.0012\n",
      "loss at iter 880: 0.0012\n",
      "loss at iter 881: 0.0012\n",
      "loss at iter 882: 0.0012\n",
      "loss at iter 883: 0.0012\n",
      "loss at iter 884: 0.0012\n",
      "loss at iter 885: 0.0011\n",
      "loss at iter 886: 0.0011\n",
      "loss at iter 887: 0.0011\n",
      "loss at iter 888: 0.0011\n",
      "loss at iter 889: 0.0011\n",
      "loss at iter 890: 0.0011\n",
      "loss at iter 891: 0.0011\n",
      "loss at iter 892: 0.0011\n",
      "loss at iter 893: 0.0011\n",
      "loss at iter 894: 0.0011\n",
      "loss at iter 895: 0.0011\n",
      "loss at iter 896: 0.0011\n",
      "loss at iter 897: 0.0011\n",
      "loss at iter 898: 0.0011\n",
      "loss at iter 899: 0.0011\n",
      "loss at iter 900: 0.0011\n",
      "loss at iter 901: 0.0011\n",
      "loss at iter 902: 0.0011\n",
      "loss at iter 903: 0.0011\n",
      "loss at iter 904: 0.0011\n",
      "loss at iter 905: 0.0011\n",
      "loss at iter 906: 0.0011\n",
      "loss at iter 907: 0.0011\n",
      "loss at iter 908: 0.0011\n",
      "loss at iter 909: 0.0011\n",
      "loss at iter 910: 0.0011\n",
      "loss at iter 911: 0.0011\n",
      "loss at iter 912: 0.0011\n",
      "loss at iter 913: 0.0011\n",
      "loss at iter 914: 0.0011\n",
      "loss at iter 915: 0.0011\n",
      "loss at iter 916: 0.0011\n",
      "loss at iter 917: 0.0011\n",
      "loss at iter 918: 0.0011\n",
      "loss at iter 919: 0.0011\n",
      "loss at iter 920: 0.0011\n",
      "loss at iter 921: 0.0011\n",
      "loss at iter 922: 0.0011\n",
      "loss at iter 923: 0.0010\n",
      "loss at iter 924: 0.0010\n",
      "loss at iter 925: 0.0010\n",
      "loss at iter 926: 0.0010\n",
      "loss at iter 927: 0.0010\n",
      "loss at iter 928: 0.0010\n",
      "loss at iter 929: 0.0010\n",
      "loss at iter 930: 0.0010\n",
      "loss at iter 931: 0.0010\n",
      "loss at iter 932: 0.0010\n",
      "loss at iter 933: 0.0010\n",
      "loss at iter 934: 0.0010\n",
      "loss at iter 935: 0.0010\n",
      "loss at iter 936: 0.0010\n",
      "loss at iter 937: 0.0010\n",
      "loss at iter 938: 0.0010\n",
      "loss at iter 939: 0.0010\n",
      "loss at iter 940: 0.0010\n",
      "loss at iter 941: 0.0010\n",
      "loss at iter 942: 0.0010\n",
      "loss at iter 943: 0.0010\n",
      "loss at iter 944: 0.0010\n",
      "loss at iter 945: 0.0010\n",
      "loss at iter 946: 0.0010\n",
      "loss at iter 947: 0.0010\n",
      "loss at iter 948: 0.0010\n",
      "loss at iter 949: 0.0010\n",
      "loss at iter 950: 0.0010\n",
      "loss at iter 951: 0.0010\n",
      "loss at iter 952: 0.0010\n",
      "loss at iter 953: 0.0010\n",
      "loss at iter 954: 0.0010\n",
      "loss at iter 955: 0.0010\n",
      "loss at iter 956: 0.0010\n",
      "loss at iter 957: 0.0010\n",
      "loss at iter 958: 0.0010\n",
      "loss at iter 959: 0.0010\n",
      "loss at iter 960: 0.0010\n",
      "loss at iter 961: 0.0010\n",
      "loss at iter 962: 0.0010\n",
      "loss at iter 963: 0.0010\n",
      "loss at iter 964: 0.0010\n",
      "loss at iter 965: 0.0010\n",
      "loss at iter 966: 0.0010\n",
      "loss at iter 967: 0.0010\n",
      "loss at iter 968: 0.0010\n",
      "loss at iter 969: 0.0010\n",
      "loss at iter 970: 0.0010\n",
      "loss at iter 971: 0.0009\n",
      "loss at iter 972: 0.0009\n",
      "loss at iter 973: 0.0009\n",
      "loss at iter 974: 0.0009\n",
      "loss at iter 975: 0.0009\n",
      "loss at iter 976: 0.0009\n",
      "loss at iter 977: 0.0009\n",
      "loss at iter 978: 0.0009\n",
      "loss at iter 979: 0.0009\n",
      "loss at iter 980: 0.0009\n",
      "loss at iter 981: 0.0009\n",
      "loss at iter 982: 0.0009\n",
      "loss at iter 983: 0.0009\n",
      "loss at iter 984: 0.0009\n",
      "loss at iter 985: 0.0009\n",
      "loss at iter 986: 0.0009\n",
      "loss at iter 987: 0.0009\n",
      "loss at iter 988: 0.0009\n",
      "loss at iter 989: 0.0009\n",
      "loss at iter 990: 0.0009\n",
      "loss at iter 991: 0.0009\n",
      "loss at iter 992: 0.0009\n",
      "loss at iter 993: 0.0009\n",
      "loss at iter 994: 0.0009\n",
      "loss at iter 995: 0.0009\n",
      "loss at iter 996: 0.0009\n",
      "loss at iter 997: 0.0009\n",
      "loss at iter 998: 0.0009\n",
      "loss at iter 999: 0.0009\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "input_x = tf.placeholder('float32', shape=[None, 3], name='x')\n",
    "input_y = tf.placeholder('float32', shape=[None], name='y')\n",
    "\n",
    "weights = tf.Variable(initial_value=np.random.rand(3, 1) * 0.01, dtype='float32')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "predicted_y = tf.matmul(input_x, weights)[:, 0]\n",
    "loss = tf.reduce_mean((input_y - predicted_y) ** 2)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "xx = x + 0.01 * np.random.normal(size=100)\n",
    "X_train = np.vstack((np.ones(100), xx, xx ** 2)).T\n",
    "y_train = x ** 2 + x + 1\n",
    "\n",
    "x = np.linspace(1, 1.3, 30)\n",
    "X_test = np.vstack((np.ones(30), x, x ** 2)).T\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    loss_i, _ = sess.run([loss, train_step], {input_x: X_train, input_y: y_train})\n",
    "    print(\"loss at iter %i: %.4f\" % (i, loss_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = sess.run(predicted_y, {input_x: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.95023203,  2.97963476,  3.00922942,  3.03901625,  3.06899524,\n",
       "        3.09916639,  3.12952924,  3.16008472,  3.19083166,  3.22177076,\n",
       "        3.25290203,  3.28422546,  3.31574106,  3.34744883,  3.37934828,\n",
       "        3.41144037,  3.44372392,  3.47619963,  3.50886774,  3.54172754,\n",
       "        3.57477951,  3.60802364,  3.64145994,  3.67508841,  3.70890903,\n",
       "        3.74292135,  3.77712584,  3.81152248,  3.8461113 ,  3.8808918 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1210cca20>,\n",
       " <matplotlib.lines.Line2D at 0x1210ccbe0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,1.3,30)\n",
    "plt.plot(x, x ** 2 + x + 1, x, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01467502]\n",
      " [ 1.03813994]\n",
      " [ 0.89741719]]\n"
     ]
    }
   ],
   "source": [
    "print(weights.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5zlmhb8b\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp5zlmhb8b', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6b0371080>, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_log_step_count_steps': 100, '_eval_distribute': None, '_task_id': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_global_id_in_cluster': 0, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_master': '', '_keep_checkpoint_max': 5, '_experimental_distribute': None, '_is_chief': True, '_protocol': None, '_save_checkpoints_secs': 600, '_task_type': 'worker', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_evaluation_master': '', '_train_distribute': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Estimator.predict at 0x7ff69da298e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(2, return_X_y=True)\n",
    "\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
